{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b89d1a5-5fca-4787-872d-179f85c111fe",
   "metadata": {},
   "source": [
    "# Test analyse xgboost √† partir de graphe r√©el "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c2c0d1a-4cb3-40af-9173-060975a6ab08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import html\n",
    "import io\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0778527e-5534-463e-b03f-8b5cdfbc41ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "np.random.seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1abe3df6-8ed7-42e9-8d80-e5ac79d1216b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Graphe charg√© : 3363 n≈ìuds et 13547 liens.\n",
      "Graphe sauvegard√© dans outputs/graphs/Airports.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bgdu69/miniconda3/envs/stageM2/lib/python3.10/site-packages/networkx/readwrite/json_graph/node_link.py:142: FutureWarning: \n",
      "The default value will be `edges=\"edges\" in NetworkX 3.6.\n",
      "\n",
      "To make this warning go away, explicitly set the edges kwarg, e.g.:\n",
      "\n",
      "  nx.node_link_data(G, edges=\"links\") to preserve current behavior, or\n",
      "  nx.node_link_data(G, edges=\"edges\") for forward compatibility.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def load_graphml_safe(path):\n",
    "    # 1. Lire le fichier brut\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        raw_data = f.read()\n",
    "    \n",
    "    # 2. Convertir les entit√©s HTML (M&Eacute;XICO -> M√âXICO)\n",
    "    # Cela √©vite l'erreur de parsing XML\n",
    "    clean_data = html.unescape(raw_data)\n",
    "    \n",
    "    # 3. Charger dans NetworkX via un flux texte\n",
    "    G = nx.read_graphml(io.StringIO(clean_data))\n",
    "    \n",
    "    print(f\"‚úÖ Graphe charg√© : {G.number_of_nodes()} n≈ìuds et {G.number_of_edges()} liens.\")\n",
    "    return G\n",
    "\n",
    "# Utilisation\n",
    "G_real = load_graphml_safe(\"outputs/graphs/Airports.graphml\")\n",
    "\n",
    "class GraphEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, set):\n",
    "            return list(obj)\n",
    "        return super().default(obj)\n",
    "\n",
    "def save_graph(G, filename):\n",
    "    data = nx.node_link_data(G)\n",
    "    with open(filename, 'w') as f:\n",
    "        # On utilise notre encodeur personnalis√© ici\n",
    "        json.dump(data, f, cls=GraphEncoder)\n",
    "    print(f\"Graphe sauvegard√© dans {filename}\")\n",
    "\n",
    "save_graph(G_real, \"outputs/graphs/Airports.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152c508b-8751-4a85-8fb9-019a4417c3a0",
   "metadata": {},
   "source": [
    "## 0 - Explo des attributs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4fea982d-7e52-4047-bc7f-3f2608a04d2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample node attributes:\n",
      "('0', {'lon': -145.50972222222222, 'lat': -17.35388888888889, 'population': 10000, 'country': 'FRENCH_POLYNESIA', 'city_name': 'Anaa'})\n"
     ]
    }
   ],
   "source": [
    "sample_node = list(G_real.nodes(data=True))[0]\n",
    "print(\"\\nSample node attributes:\")\n",
    "print(sample_node)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff40a3e-9126-46ec-a721-80a717facccd",
   "metadata": {},
   "source": [
    "## 1 - Calcul des attributs de noeuds et de paires de noeuds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a96dff6-ddc3-4f41-914e-37a4e2c88144",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topology_features(G, u, v, precomputed, is_existing_edge=False):\n",
    "    \"\"\"Calcule les m√©triques topologiques pour une paire (u, v)\"\"\"\n",
    "    \n",
    "    # 1. M√©triques de paires (Voisinage)\n",
    "    aa = next(nx.adamic_adar_index(G, [(u, v)]))[2]\n",
    "    jc = next(nx.jaccard_coefficient(G, [(u, v)]))[2]\n",
    "    pa = next(nx.preferential_attachment(G, [(u, v)]))[2]\n",
    "    cn = len(list(nx.common_neighbors(G, u, v)))\n",
    "\n",
    "    try:\n",
    "        sp = nx.shortest_path_length(G, source=u, target=v)\n",
    "    except nx.NetworkXNoPath:\n",
    "        sp = 0 \n",
    "\n",
    "    # 2. M√©triques de N≈ìuds (extraites du dictionnaire pr√©-calcul√©)\n",
    "    # On ajoute les versions pour u et pour v\n",
    "    node_features = {\n",
    "        'pr_u': precomputed['pr'].get(u, 0), 'pr_v': precomputed['pr'].get(v, 0),\n",
    "        'lcc_u': precomputed['lcc'].get(u, 0), 'lcc_v': precomputed['lcc'].get(v, 0),\n",
    "        'and_u': precomputed['and'].get(u, 0), 'and_v': precomputed['and'].get(v, 0),\n",
    "        'dc_u': precomputed['dc'].get(u, 0), 'dc_v': precomputed['dc'].get(v, 0)\n",
    "    }\n",
    "\n",
    "    # Fusion de toutes les m√©triques\n",
    "    topo_res = {'cn': cn, 'aa': aa, 'jc': jc, 'pa': pa, \n",
    "                'sp': sp\n",
    "               }\n",
    "    topo_res.update(node_features)\n",
    "    \n",
    "    return topo_res\n",
    "\n",
    "def prepare_balanced_data_unknown_pos_and_community(G, test_size = 0.15, negative_ratio=1.0):\n",
    "    all_edges = list(G.edges())\n",
    "    nodes = list(G.nodes())\n",
    "    n_pos = len(all_edges)\n",
    "    data = []\n",
    "    random.seed(42)\n",
    "\n",
    "    # 1. Extraction des ar√™tes pour le split\n",
    "    random.shuffle(all_edges)\n",
    "    \n",
    "    split_idx = int(len(all_edges) * (1 - test_size))\n",
    "    train_edges = all_edges[:split_idx]\n",
    "    test_edges = all_edges[split_idx:]\n",
    "    \n",
    "    # 2. Cr√©ation du graphe d'entra√Ænement (G sans le test set)\n",
    "    # C'est sur ce graphe qu'on va tout calculer\n",
    "    G_train = nx.Graph()\n",
    "    G_train.add_nodes_from(G.nodes())\n",
    "    G_train.add_edges_from(train_edges)\n",
    "    \n",
    "    print(f\"Graphe original: {G.number_of_edges()} liens\")\n",
    "    print(f\"Graphe d'entra√Ænement: {G_train.number_of_edges()} liens\")\n",
    "    print(f\"Liens cach√©s pour le test: {len(test_edges)}\")\n",
    "    \n",
    "\n",
    "    # --- √âTAPE DE PR√â-CALCUL ---\n",
    "    # On calcule les m√©triques de noeuds une seule fois ici\n",
    "    print(\"Pr√©-calcul des m√©triques de n≈ìuds...\")\n",
    "    precomputed = {\n",
    "        'pr': nx.pagerank(G_train),                    # PageRank (PR)\n",
    "        'lcc': nx.clustering(G_train),                # Local Clustering Coefficient (LCC)\n",
    "        'and': nx.average_neighbor_degree(G_train),   # Average Neighbor Degree (AND)\n",
    "        'dc': nx.degree_centrality(G_train)           # Degree Centrality (DC)\n",
    "    }\n",
    "    \n",
    "    # --- 1. CLASSE POSITIVE ---\n",
    "    for u, v in all_edges:\n",
    "        topo = get_topology_features(G_train, u, v, precomputed, is_existing_edge=True)\n",
    "        \n",
    "        row = {\n",
    "            'u': u, \n",
    "            'v': v,\n",
    "            'target': 1\n",
    "        }\n",
    "        row.update(topo)\n",
    "        data.append(row)\n",
    "    \n",
    "    # --- 2. CLASSE N√âGATIVE ---\n",
    "    n_neg_target = int(n_pos * negative_ratio)\n",
    "    neg_count = 0\n",
    "    while neg_count < n_neg_target:\n",
    "        u, v = random.sample(nodes, 2)\n",
    "        if not G.has_edge(u, v) and u != v:\n",
    "            topo = get_topology_features(G_train, u, v, precomputed, is_existing_edge=False)\n",
    "            \n",
    "            row = {\n",
    "                'u': u, \n",
    "                'v': v,\n",
    "                'target': 0\n",
    "            }\n",
    "            row.update(topo)\n",
    "            data.append(row)\n",
    "            neg_count += 1\n",
    "\n",
    "    print(f\"DataFrame cr√©√© <3 : {len(data)} paires de noeuds choisies\")\n",
    "    return pd.DataFrame(data), G_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d1a1d50f-8cce-41e1-8915-aaa8dc7f039d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graphe original: 13547 liens\n",
      "Graphe d'entra√Ænement: 11514 liens\n",
      "Liens cach√©s pour le test: 2033\n",
      "Pr√©-calcul des m√©triques de n≈ìuds...\n",
      "DataFrame cr√©√© <3 : 27094 paires de noeuds choisies\n"
     ]
    }
   ],
   "source": [
    "heuristics_only_data, G_train = prepare_balanced_data_unknown_pos_and_community(G_real)\n",
    "heuristics_only_data.to_parquet(\"outputs/datasets/Airports_heuristics_only_dataset.parquet\", engine='pyarrow')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "976e7b43-edef-4692-a8b8-b48fc650b923",
   "metadata": {},
   "source": [
    "## 2 - Inf√©rences sur Position & Communities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff7fa35a-1f85-421b-b43a-37cf8614aefe",
   "metadata": {},
   "source": [
    "#### DeepWalk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384e6c54-fdc4-411d-83d7-1874d31c813e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(heuristics_only_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ba9df4-2ce9-41d9-8f32-825c6277a8fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "762354cf-5862-4d0e-8c2a-7875ed0656af",
   "metadata": {},
   "source": [
    "#### Node2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae0f791-b877-46c6-bfdd-101ab1040d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from node2vec import Node2Vec\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91526a52-d99d-4910-bac5-4100aaf9bab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_node2vec_features(G, dimensions=64):\n",
    "    \"\"\"\n",
    "    G√©n√®re les embeddings Node2Vec et retourne un dictionnaire {node_id: vector}\n",
    "    \"\"\"\n",
    "    print(f\"üöÄ G√©n√©ration des marches al√©atoires (dim={dimensions})...\")\n",
    "    \n",
    "    # Configuration de Node2Vec\n",
    "    # p=1, q=1 -> √©quivalent √† DeepWalk\n",
    "    # p=1, q=2 -> Favorise l'exploration locale (structure)\n",
    "    # p=2, q=0.5 -> Favorise l'exploration lointaine (communaut√©s) - homophilie\n",
    "    node2vec = Node2Vec(G, \n",
    "                        dimensions=dimensions, \n",
    "                        walk_length=30, \n",
    "                        num_walks=100, \n",
    "                        workers=4, \n",
    "                        p=2, q=0.5)\n",
    "\n",
    "    print(\"üß† Entra√Ænement du mod√®le Skip-gram...\")\n",
    "    model = node2vec.fit(window=10, min_count=1, batch_words=4)\n",
    "    \n",
    "    # On r√©cup√®re les vecteurs dans un dictionnaire\n",
    "    embeddings = {str(node): model.wv[str(node)] for node in G.nodes()}\n",
    "    return embeddings\n",
    "\n",
    "def add_node2vec_to_df(df, embeddings):\n",
    "    \"\"\"\n",
    "    Ajoute des features de similarit√© bas√©es sur Node2Vec au DataFrame\n",
    "    \"\"\"\n",
    "    print(\"üìä Calcul des distances vectorielles pour chaque paire...\")\n",
    "    \n",
    "    def get_cosine_sim(u, v):\n",
    "        vec_u = embeddings[str(u)].reshape(1, -1)\n",
    "        vec_v = embeddings[str(v)].reshape(1, -1)\n",
    "        return cosine_similarity(vec_u, vec_v)[0][0]\n",
    "\n",
    "    def get_l2_dist(u, v):\n",
    "        vec_u = embeddings[str(u)]\n",
    "        vec_v = embeddings[str(v)]\n",
    "        return np.linalg.norm(vec_u - vec_v)\n",
    "\n",
    "    # Similarit√© Cosinus : 1 = tr√®s proche, 0 = perpendiculaire\n",
    "    df['n2v_cosine'] = df.apply(lambda row: get_cosine_sim(row['u'], row['v']), axis=1)\n",
    "    \n",
    "    # Distance Euclidienne : plus c'est petit, plus ils sont proches\n",
    "    df['n2v_dist'] = df.apply(lambda row: get_l2_dist(row['u'], row['v']), axis=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# --- UTILISATION ---\n",
    "# 1. Calcul des vecteurs d'embedding\n",
    "n2v_embeddings = compute_node2vec_features(G_train)\n",
    "\n",
    "# 2. Injection dans le DataFrame de training/test\n",
    "node2vec_homophilie_p2_q0_5_data = add_node2vec_to_df(heuristics_only_data, n2v_embeddings)\n",
    "node2vec_homophilie_p2_q0_5_data.to_parquet(\n",
    "    \"outputs/datasets/Airports_node2vec_homophilie_p2_q0_5_dataset.parquet\", \n",
    "    engine='pyarrow'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dead6f1f-329d-4df3-ae27-bf576e881508",
   "metadata": {},
   "source": [
    "#### Role2Vec Tentative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54cfe3eb-46ad-461a-a567-d3afbeff4d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from karateclub import Role2Vec\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed29b30-b1c9-4758-9a84-e486393adea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Karate Club n√©cessite que les n≈ìuds soient index√©s par des entiers de 0 √† N-1\n",
    "mapping = {node: i for i, node in enumerate(G_real.nodes())}\n",
    "reverse_mapping = {i: node for node, i in mapping.items()}\n",
    "G_reindexed = nx.relabel_nodes(G_real, mapping)\n",
    "\n",
    "# 2. Entra√Ænement de Role2Vec\n",
    "print(\"üé≠ Calcul de Role2Vec (Roles structurels)...\")\n",
    "# On reste sur des dimensions coh√©rentes avec tes autres tests\n",
    "model_r2v = Role2Vec(dimensions=64, walk_number=10, walk_length=80) \n",
    "model_r2v.fit(G_reindexed)\n",
    "\n",
    "# 3. Extraction des vecteurs\n",
    "r2v_embeddings_raw = model_r2v.get_embedding()\n",
    "r2v_embeddings = {reverse_mapping[i]: r2v_embeddings_raw[i] for i in range(len(G_real))}\n",
    "\n",
    "print(\"‚úÖ Role2Vec termin√© !\")\n",
    "\n",
    "role2vec_data = add_node2vec_to_df(heuristics_only_data, r2v_embeddings)\n",
    "role2vec_data.to_parquet(\n",
    "    \"outputs/datasets/Airports_role2vec_dataset.parquet\", \n",
    "    engine='pyarrow'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7b4a4d-a86d-412f-90bd-a5d4726e0d83",
   "metadata": {},
   "source": [
    "#### Louvain Communities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc24c9dc-1a20-411e-89cb-612d877c3d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from networkx.algorithms.community import louvain_communities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2dc25b-b0bc-4b68-9854-778008dd8032",
   "metadata": {},
   "outputs": [],
   "source": [
    "communities = nx.community.louvain_communities(G_train, seed=42)\n",
    "\n",
    "node_to_community = {} \n",
    "for i, community in enumerate(communities):\n",
    "    for node in community:\n",
    "        node_to_community[node] = i\n",
    "        \n",
    "print(len(node_to_community))\n",
    "\n",
    "heuristics_only_data = pd.read_parquet(\"outputs/datasets/Airports_heuristics_only_dataset.parquet\")\n",
    "print(type(heuristics_only_data))\n",
    "louvain_communities_data = heuristics_only_data.copy()\n",
    "\n",
    "louvain_communities_data[\"community_u\"] = louvain_communities_data[\"u\"].map(node_to_community)\n",
    "louvain_communities_data[\"community_v\"] = louvain_communities_data[\"v\"].map(node_to_community)\n",
    "louvain_communities_data[\"same_community\"] = (louvain_communities_data[\"community_u\"] == louvain_communities_data[\"community_v\"]).astype(int)\n",
    "\n",
    "louvain_communities_data.to_parquet(\n",
    "    \"outputs/datasets/Airports_louvain_communities_dataset.parquet\", \n",
    "    engine='pyarrow'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5155a5e8-1a6d-454a-a87e-2b4e6dbe929b",
   "metadata": {},
   "source": [
    "#### Infomap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7bd9315-4246-49af-b623-ae0c179498c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from infomap import Infomap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d4927f-ecf0-4efa-85ef-74b9483dd126",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_infomap_communities(G):\n",
    "    im = Infomap(\"--two-level --silent\")\n",
    "    \n",
    "    for source, target in G.edges():\n",
    "        im.add_link(int(source), int(target))\n",
    "    \n",
    "    # Ex√©cution\n",
    "    im.run()\n",
    "    \n",
    "    # Extraction des r√©sultats\n",
    "    # node.node_id est l'ID d'origine, node.module_id est le cluster\n",
    "    node_to_infomap = {node.node_id: node.module_id for node in im.tree if node.is_leaf}\n",
    "    return node_to_infomap\n",
    "\n",
    "# Utilisation\n",
    "infomap_dict = get_infomap_communities(G_train)\n",
    "print(f\"Infomap a trouv√© {len(set(infomap_dict.values()))} communaut√©s.\")\n",
    "\n",
    "infomap_data = heuristics_only_data.copy()\n",
    "\n",
    "# 2. Mapping avec le dictionnaire Infomap\n",
    "infomap_data[\"u\"] = pd.to_numeric(infomap_data[\"u\"], errors='coerce').astype(int)\n",
    "infomap_data[\"v\"] = pd.to_numeric(infomap_data[\"v\"], errors='coerce').astype(int)\n",
    "\n",
    "infomap_data[\"infomap_u\"] = infomap_data[\"u\"].map(infomap_dict)\n",
    "infomap_data[\"infomap_v\"] = infomap_data[\"v\"].map(infomap_dict)\n",
    "infomap_data[\"same_infomap\"] = (infomap_data[\"infomap_u\"] == infomap_data[\"infomap_v\"]).astype(int)\n",
    "\n",
    "print(type(infomap_dict))\n",
    "print(infomap_data.head(3))\n",
    "\n",
    "print(f\"Type cl√© dico: {type(list(infomap_dict.keys())[0])}\")\n",
    "\n",
    "# V√©rifie le type de la colonne du DataFrame\n",
    "print(f\"Type colonne DF: {infomap_data['u'].dtype}\")\n",
    "# 4. Sauvegarde\n",
    "infomap_data.to_parquet(\"outputs/datasets/Airports_infomap_dataset.parquet\", engine='pyarrow')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dadf8ad4-bf97-4958-b3d8-ebc293f06837",
   "metadata": {},
   "source": [
    "#### SBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d31d326-940e-4fb4-bf7f-6dc2d3f13307",
   "metadata": {},
   "outputs": [],
   "source": [
    "import graph_tool.all as gt\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d2be05e7-27fd-49ec-9a78-bdac55021c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def appendGraphToolSBM(G_nx, dataFrame):\n",
    "    \"\"\"\n",
    "    Inf√©rence SBM via graph-tool avec d√©tection automatique \n",
    "    du nombre de blocs (MDL).\n",
    "    \"\"\"\n",
    "    # 1. Conversion NetworkX -> Graph-tool\n",
    "    # On cr√©e un mapping pour ne pas perdre l'ordre des noeuds\n",
    "    nodes_list = list(G_nx.nodes())\n",
    "    node_index = {node: i for i, node in enumerate(nodes_list)}\n",
    "    \n",
    "    G_gt = gt.Graph(directed=False)\n",
    "    G_gt.add_vertex(len(nodes_list))\n",
    "    \n",
    "    edges = [(node_index[u], node_index[v]) for u, v in G_nx.edges()]\n",
    "    G_gt.add_edge_list(edges)\n",
    "\n",
    "    print(\"Bevor inference\")\n",
    "    # 2. Inf√©rence du mod√®le (SBM avec correction de degr√©s)\n",
    "    # 'minimize_blockmodel_dl' cherche la structure la plus simple qui explique le mieux le graphe\n",
    "    state = gt.minimize_blockmodel_dl(G_gt)\n",
    "\n",
    "    print(\"Sp√§ter inference\")\n",
    "    \n",
    "    # 3. R√©cup√©ration des blocs (communaut√©s)\n",
    "    # b est un PropertyMap qui contient l'index du bloc pour chaque sommet\n",
    "    blocks = state.get_blocks()\n",
    "    \n",
    "    # Mapping final : Nom du noeud -> ID du bloc\n",
    "    node_to_community = {nodes_list[i]: int(blocks[i]) for i in range(len(nodes_list))}\n",
    "            \n",
    "    # 4. Enrichissement du DataFrame\n",
    "    sbm_data = dataFrame.copy()\n",
    "    sbm_data[\"community_u\"] = sbm_data[\"u\"].map(node_to_community)\n",
    "    sbm_data[\"community_v\"] = sbm_data[\"v\"].map(node_to_community)\n",
    "    sbm_data[\"same_community\"] = (sbm_data[\"community_u\"] == sbm_data[\"community_v\"]).astype(int)\n",
    "    \n",
    "    return sbm_data, node_to_community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0e98eff5-e452-4aa0-9780-8e346b64ca3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bevor inference\n",
      "Sp√§ter inference\n",
      "      u     v  target  cn        aa        jc    pa  sp      pr_u      pr_v  \\\n",
      "0   230   443       1   3  0.862251  0.061224   651   1  0.000900  0.001063   \n",
      "1    99   132       1  26  6.434280  0.208000  5100   1  0.002918  0.001434   \n",
      "2  2787  2788       1   1  0.360674  0.250000     6   1  0.000166  0.000264   \n",
      "3  2256  2263       1   3  0.746942  0.500000    20   1  0.000143  0.000165   \n",
      "4   112   124       1  32  8.565122  0.310680  4004   1  0.001070  0.002243   \n",
      "\n",
      "      lcc_u     lcc_v      and_u      and_v      dc_u      dc_v  community_u  \\\n",
      "0  0.322581  0.133333  27.483871  19.333333  0.009221  0.006246          450   \n",
      "1  0.187475  0.317647  50.160000  51.941176  0.029744  0.015170          725   \n",
      "2  1.000000  0.333333   9.500000   6.333333  0.000595  0.000892         2869   \n",
      "3  0.666667  0.700000  45.750000  60.200000  0.001190  0.001487         2238   \n",
      "4  0.581395  0.206593  62.068182  40.967033  0.013087  0.027067          193   \n",
      "\n",
      "   community_v  same_community  \n",
      "0         2138               0  \n",
      "1          193               0  \n",
      "2         2869               1  \n",
      "3         2238               1  \n",
      "4          193               1  \n",
      "3363\n"
     ]
    }
   ],
   "source": [
    "sbm_data, node_to_community = appendGraphToolSBM(G_train, heuristics_only_data)\n",
    "\n",
    "print(sbm_data.head(5))\n",
    "print(len(node_to_community))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7690094d-e57c-4c23-9626-65c42b29b10f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de communaut√©s trouv√©es : 33\n"
     ]
    }
   ],
   "source": [
    "nb_comm = len(set(node_to_community.values()))\n",
    "print(f\"Nombre de communaut√©s trouv√©es : {nb_comm}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0964fd24-7c75-4275-bf02-c5d40573649f",
   "metadata": {},
   "source": [
    "## 3 - Comparaison des perfs sur XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4084c9a1-42ab-443a-8b49-70c5964c27bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import f1_score, confusion_matrix, roc_auc_score, average_precision_score\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75bdf93-2b51-4d7e-af46-f440cb67b882",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement des datasets au format Parquet\n",
    "heuristics_only_data = pd.read_parquet(\"outputs/datasets/Airports_heuristics_only_dataset.parquet\")\n",
    "deepwalk_data = pd.read_parquet(\"outputs/datasets/Airports_deepwalk_p1_q1_dataset.parquet\")\n",
    "node2vec_homophilie_p2_q0_5_data = pd.read_parquet(\"outputs/datasets/Airports_node2vec_homophilie_p2_q0_5_dataset.parquet\")\n",
    "node2vec_structural_p1_q2_data = pd.read_parquet(\"outputs/datasets/Airports_node2vec_structure_p1_q2_dataset.parquet\")\n",
    "#role2vec_data = pd.read_parquet(\"outputs/datasets/Airports_role2vec_dataset.parquet\")\n",
    "\n",
    "louvain_communities_data = pd.read_parquet(\"outputs/datasets/Airports_louvain_communities_dataset.parquet\")\n",
    "infomap_data = pd.read_parquet(\"outputs/datasets/Airports_infomap_dataset.parquet\")\n",
    "\n",
    "homo_and_louvain_data = node2vec_homophilie_p2_q0_5_data.copy()\n",
    "\n",
    "homo_and_louvain_data[\"community_u\"] = homo_and_louvain_data[\"u\"].map(node_to_community)\n",
    "homo_and_louvain_data[\"community_v\"] = homo_and_louvain_data[\"v\"].map(node_to_community)\n",
    "homo_and_louvain_data[\"same_community\"] = (homo_and_louvain_data[\"community_u\"] == homo_and_louvain_data[\"community_v\"]).astype(int)\n",
    "\n",
    "homo_and_structural_data = node2vec_homophilie_p2_q0_5_data.copy()\n",
    "homo_and_structural_data[\"n2v_cosine_s\"] = node2vec_structural_p1_q2_data[\"n2v_cosine\"]\n",
    "homo_and_structural_data[\"n2v_dist_s\"] = node2vec_structural_p1_q2_data[\"n2v_dist\"]\n",
    "\n",
    "\n",
    "print(\"‚úÖ Tous les datasets ont √©t√© charg√©s avec succ√®s depuis le format Parquet.\")\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "my_bench_datasets = {\n",
    "    \"DeepWalk\": deepwalk_data,\n",
    "    \"N2V Homophilie\": node2vec_homophilie_p2_q0_5_data,\n",
    "    \"N2V Structural\": node2vec_structural_p1_q2_data,\n",
    "    #\"Role2Vec\": role2vec_data,\n",
    "    \"Louvain Communities\": louvain_communities_data,\n",
    "    \"Infomap Communities\": infomap_data,\n",
    "    \"Homophilie and Louvain\" : homo_and_louvain_data,\n",
    "    \"Homophilie & Structural\" : homo_and_structural_data,\n",
    "    \"Heuristics Only\": heuristics_only_data\n",
    "}\n",
    "\n",
    "for name, dataset in my_bench_datasets.items():\n",
    "    print(f\"\\n--- Dataset: {name} ---\")\n",
    "    print(dataset.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1689cf31-b7e5-4b07-b6d7-d8b8cf57165b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, confusion_matrix, roc_auc_score, average_precision_score\n",
    "\n",
    "def run_benchmark(datasets_dict, K=50):\n",
    "\n",
    "    \n",
    "    results = []\n",
    "    pa_analysis = []\n",
    "    \n",
    "    # Colonnes techniques √† exclure\n",
    "    exclude = {'u', 'v', 'target'}\n",
    "    \n",
    "    for name, df in datasets_dict.items():\n",
    "        print(f\"Traitement de {name}...\")\n",
    "        \n",
    "        features = [c for c in df.columns if c not in exclude]\n",
    "        df_tmp = df.dropna(subset=features + ['target'])\n",
    "        X = df_tmp[features]\n",
    "        y = df_tmp['target']\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=0.2, random_state=42, stratify=y\n",
    "        )\n",
    "        \n",
    "        model = XGBClassifier(\n",
    "            n_estimators=100,\n",
    "            learning_rate=0.1,\n",
    "            max_depth=6,\n",
    "            eval_metric='logloss',\n",
    "            tree_method='hist',\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        \n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Pr√©dictions\n",
    "        probs = model.predict_proba(X_test)[:, 1]\n",
    "        preds = model.predict(X_test)  # Seuil par d√©faut √† 0.5\n",
    "\n",
    "       \n",
    "        # M√©triques de performance\n",
    "        auc_roc = roc_auc_score(y_test, probs)\n",
    "        f1 = f1_score(y_test, preds)\n",
    "        \n",
    "        # Matrice de confusion\n",
    "        tn, fp, fn, tp = confusion_matrix(y_test, preds).ravel()\n",
    "        \n",
    "        # Hits@K\n",
    "        top_k_indices = np.argsort(probs)[-K:]\n",
    "        hits_at_k = y_test.iloc[top_k_indices].sum() / K\n",
    "\n",
    "        # --- ANALYSE D√âTAILL√âE PAR CN (VOISINS COMMUNS) ---\n",
    "        df_test_analysis = pd.DataFrame({\n",
    "            'cn_val': X_test['cn'].values,\n",
    "            'target': y_test.values,\n",
    "            'pred': preds\n",
    "        })\n",
    "\n",
    "        print(f\"\\n--- D√©tails par CN pour le dataset : {name} ---\")\n",
    "        \n",
    "        # On it√®re sur chaque valeur unique de voisins communs\n",
    "        for cn_val in sorted(df_test_analysis['cn_val'].unique()):\n",
    "            subset = df_test_analysis[df_test_analysis['cn_val'] == cn_val]\n",
    "            \n",
    "            y_true_sub = subset['target']\n",
    "            y_pred_sub = subset['pred']\n",
    "            \n",
    "            # Calcul du F1 local\n",
    "            if len(np.unique(y_true_sub)) > 1:\n",
    "                f1_sub = f1_score(y_true_sub, y_pred_sub)\n",
    "            else:\n",
    "                # Cas o√π il n'y a qu'une seule classe (ex: que des 0 ou que des 1)\n",
    "                f1_sub = 1.0 if (y_true_sub == y_pred_sub).all() else 0.0\n",
    "            \n",
    "            n_pos = int(y_true_sub.sum())\n",
    "            n_neg = len(subset) - n_pos\n",
    "            \n",
    "            print(f\"CN: {int(cn_val):<2} | Total: {len(subset):<5} | Pos: {n_pos:<4} | Neg: {n_neg:<4} | F1: {f1_sub:.4f}\")\n",
    "        \n",
    "        results.append({\n",
    "            'Dataset': name,\n",
    "            'AUC-ROC': round(auc_roc, 4),\n",
    "            'F1-Score': round(f1, 4),\n",
    "            'VP (True Pos)': tp,\n",
    "            'VN (True Neg)': tn,\n",
    "            'FP (False Pos)': fp,\n",
    "            'FN (False Neg)': fn,\n",
    "            f'Hits@{K}': round(hits_at_k, 4)\n",
    "        })\n",
    "        \n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    # On r√©cup√®re le dernier mod√®le entra√Æn√© dans votre boucle\n",
    "    # (Assurez-vous de l'avoir entra√Æn√© sur le 'N2V Homophilie')\n",
    "    \n",
    "    importance = model.feature_importances_\n",
    "    feat_names = features # La liste des colonnes utilis√©e pour X\n",
    "    \n",
    "    # Cr√©ation d'un DataFrame pour trier\n",
    "    df_imp = pd.DataFrame({'feature': feat_names, 'importance': importance})\n",
    "    df_imp = df_imp.sort_values(by='importance', ascending=False).head(15)\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(df_imp['feature'], df_imp['importance'], color='skyblue')\n",
    "    plt.xlabel(\"Importance (Gain)\")\n",
    "    plt.title(\"Top 15 Features - N2V Homophilie\")\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.show()\n",
    "\n",
    "    return pd.DataFrame(results).sort_values(by='AUC-ROC', ascending=False)\n",
    "    \n",
    "\n",
    "# --- EXECUTION ---\n",
    "summary = run_benchmark(my_bench_datasets)\n",
    "print(\"\\n R√âSULTATS COMPARATIFS\")\n",
    "print(summary.to_string(index=False))\n",
    "summary.to_csv(\"outputs/plots/resultats_comparatifs_sp.csv\", index=False, sep=';', encoding='utf-8')\n",
    "print(my_bench_datasets[\"Heuristics Only\"].groupby('target')[['pr_u', 'pr_v']].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4302fbd-2ae4-42c6-9e56-4db4514ab523",
   "metadata": {},
   "source": [
    "## 4 - Analyse SHAP rapide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b12a0bf-9db2-4af1-895c-2af65572c233",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be34f4dc-c3a3-4608-a2aa-db8ddcf1d93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_with_shap(model, X_test, output_dir=\"outputs/plots\"):\n",
    "    \"\"\"Calcule les SHAP values et g√©n√®re les plots globaux proprement.\"\"\"\n",
    "    # 1. Configuration de l'explainer 'Bo√Æte Noire' (le plus stable sur mon Mac)\n",
    "    # On d√©finit la fonction de pr√©diction (proba de la classe 1)\n",
    "    model_predict = lambda x: model.predict_proba(x)[:, 1]\n",
    "    \n",
    "    # Utilisation d'un masker (√©chantillon de r√©f√©rence)\n",
    "    # On prend 50 lignes pour √©quilibrer vitesse et pr√©cision\n",
    "    masker = X_test.iloc[:50]\n",
    "    \n",
    "    # Initialisation de l'explainer\n",
    "    explainer = shap.Explainer(model_predict, masker)    \n",
    "    \n",
    "    # 2. Calcul effectif des SHAP values\n",
    "    # On r√©cup√®re l'objet 'Explanation' complet\n",
    "    shap_explanation = explainer(X_test)\n",
    "    \n",
    "    # 3. Extraction des valeurs num√©riques pour le retour de fonction\n",
    "    # On r√©cup√®re les valeurs brutes (.values)\n",
    "    shap_values = shap_explanation.values\n",
    "\n",
    "    # Gestion de la dimension (si SHAP renvoie [n_samples, n_features, 2])\n",
    "    if len(shap_values.shape) == 3:\n",
    "        shap_values = shap_values[:, :, 1]\n",
    "    \n",
    "    # --- G√âN√âRATION DES PLOTS ---\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Plot 1: Summary Points (Beeswarm)\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    # On peut passer l'objet explanation directement, c'est plus moderne\n",
    "    shap.plots.beeswarm(shap_explanation, show=False)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, \"shap_summary_points.png\"))\n",
    "    plt.close()\n",
    "\n",
    "    # Plot 2: Summary Bar\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    shap.plots.bar(shap_explanation, show=False)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, \"shap_summary_bar.png\"))\n",
    "    plt.close()\n",
    "    \n",
    "    return shap_values\n",
    "\n",
    "def calculate_feature_rankings(shap_values, feature_names, output_dir=\"outputs/plots\"):\n",
    "    \"\"\"Calcule la distribution des rangs et g√©n√®re le barplot du Top 5.\"\"\"\n",
    "    abs_shap = np.abs(shap_values)\n",
    "    ranks = np.argsort(-abs_shap, axis=1)\n",
    "    \n",
    "    ranking_stats = {}\n",
    "    n_samples, n_features = shap_values.shape\n",
    "\n",
    "    for i, name in enumerate(feature_names):\n",
    "        feature_ranks = np.where(ranks == i)[1] + 1\n",
    "        counts = np.bincount(feature_ranks, minlength=n_features + 1)[1:]\n",
    "        ranking_stats[name] = (counts / n_samples) * 100\n",
    "\n",
    "    df_ranks = pd.DataFrame(ranking_stats, index=[f\"Rang {i+1}\" for i in range(n_features)])\n",
    "    \n",
    "    # Plot 3: Top 5 Appearance\n",
    "    top5 = df_ranks.iloc[0:5, :].sum(axis=0).sort_values(ascending=False)\n",
    "    plt.figure(figsize=(12, 7))\n",
    "    sns.barplot(x=top5.index, y=top5.values, palette=\"viridis\")\n",
    "    plt.title(\"Importance structurelle : % de pr√©sence dans le Top 5 SHAP\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, \"shap_top5_frequency.png\"))\n",
    "    plt.close()\n",
    "    \n",
    "    return df_ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5c88cc-321a-4a41-8789-fffe0ca4b06a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c51237-c9f0-4f35-9ac3-40540faab945",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Stage M2)",
   "language": "python",
   "name": "stagem2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
