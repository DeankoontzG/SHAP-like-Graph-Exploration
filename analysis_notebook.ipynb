{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b89d1a5-5fca-4787-872d-179f85c111fe",
   "metadata": {},
   "source": [
    "# Test analyse xgboost √† partir de graphe r√©el "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c2c0d1a-4cb3-40af-9173-060975a6ab08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pandas: 2.3.3\n",
      "Numpy: 1.26.4\n",
      "NetworkX: 3.4.2\n",
      "Louvain fonctionne !\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "import html\n",
    "import io\n",
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0778527e-5534-463e-b03f-8b5cdfbc41ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "np.random.seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1abe3df6-8ed7-42e9-8d80-e5ac79d1216b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Graphe charg√© : 3363 n≈ìuds et 13547 liens.\n",
      "Graphe sauvegard√© dans outputs/graphs/Airports.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bgdu69/miniconda3/envs/stageM2/lib/python3.10/site-packages/networkx/readwrite/json_graph/node_link.py:142: FutureWarning: \n",
      "The default value will be `edges=\"edges\" in NetworkX 3.6.\n",
      "\n",
      "To make this warning go away, explicitly set the edges kwarg, e.g.:\n",
      "\n",
      "  nx.node_link_data(G, edges=\"links\") to preserve current behavior, or\n",
      "  nx.node_link_data(G, edges=\"edges\") for forward compatibility.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def load_graphml_safe(path):\n",
    "    # 1. Lire le fichier brut\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        raw_data = f.read()\n",
    "    \n",
    "    # 2. Convertir les entit√©s HTML (M&Eacute;XICO -> M√âXICO)\n",
    "    # Cela √©vite l'erreur de parsing XML\n",
    "    clean_data = html.unescape(raw_data)\n",
    "    \n",
    "    # 3. Charger dans NetworkX via un flux texte\n",
    "    G = nx.read_graphml(io.StringIO(clean_data))\n",
    "    \n",
    "    print(f\"‚úÖ Graphe charg√© : {G.number_of_nodes()} n≈ìuds et {G.number_of_edges()} liens.\")\n",
    "    return G\n",
    "\n",
    "# Utilisation\n",
    "G_real = load_graphml_safe(\"outputs/graphs/Airports.graphml\")\n",
    "\n",
    "class GraphEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, set):\n",
    "            return list(obj)\n",
    "        return super().default(obj)\n",
    "\n",
    "def save_graph(G, filename):\n",
    "    data = nx.node_link_data(G)\n",
    "    with open(filename, 'w') as f:\n",
    "        # On utilise notre encodeur personnalis√© ici\n",
    "        json.dump(data, f, cls=GraphEncoder)\n",
    "    print(f\"Graphe sauvegard√© dans {filename}\")\n",
    "\n",
    "save_graph(G_real, \"outputs/graphs/Airports.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152c508b-8751-4a85-8fb9-019a4417c3a0",
   "metadata": {},
   "source": [
    "## 0 - Explo des attributs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fea982d-7e52-4047-bc7f-3f2608a04d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_node = list(G_real.nodes(data=True))[0]\n",
    "print(\"\\nSample node attributes:\")\n",
    "print(sample_node)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff40a3e-9126-46ec-a721-80a717facccd",
   "metadata": {},
   "source": [
    "## 1 - Calcul des attributs de noeuds et de paires de noeuds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "7a96dff6-ddc3-4f41-914e-37a4e2c88144",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topology_features(G, u, v, precomputed, is_existing_edge=False):\n",
    "    \"\"\"Calcule les m√©triques topologiques pour une paire (u, v)\"\"\"\n",
    "    # 1. M√©triques de paires (Voisinage)\n",
    "    aa = next(nx.adamic_adar_index(G, [(u, v)]))[2]\n",
    "    jc = next(nx.jaccard_coefficient(G, [(u, v)]))[2]\n",
    "    pa = next(nx.preferential_attachment(G, [(u, v)]))[2]\n",
    "    cn = len(list(nx.common_neighbors(G, u, v)))\n",
    "\n",
    "    # 2. M√©triques de N≈ìuds (extraites du dictionnaire pr√©-calcul√©)\n",
    "    # On ajoute les versions pour u et pour v\n",
    "    node_features = {\n",
    "        'pr_u': precomputed['pr'].get(u, 0), 'pr_v': precomputed['pr'].get(v, 0),\n",
    "        'lcc_u': precomputed['lcc'].get(u, 0), 'lcc_v': precomputed['lcc'].get(v, 0),\n",
    "        'and_u': precomputed['and'].get(u, 0), 'and_v': precomputed['and'].get(v, 0),\n",
    "        'dc_u': precomputed['dc'].get(u, 0), 'dc_v': precomputed['dc'].get(v, 0)\n",
    "    }\n",
    "\n",
    "    # 3. Shortest Path (SP)\n",
    "    if is_existing_edge:\n",
    "        G.remove_edge(u, v)\n",
    "        try:\n",
    "            sp = nx.shortest_path_length(G, source=u, target=v)\n",
    "        except nx.NetworkXNoPath:\n",
    "            sp = 0 \n",
    "        G.add_edge(u, v)\n",
    "    else:\n",
    "        try:\n",
    "            sp = nx.shortest_path_length(G, source=u, target=v)\n",
    "        except nx.NetworkXNoPath:\n",
    "            sp = 0\n",
    "\n",
    "    # Fusion de toutes les m√©triques\n",
    "    topo_res = {'cn': cn, 'aa': aa, 'jc': jc, 'pa': pa, 'sp': sp}\n",
    "    topo_res.update(node_features)\n",
    "    \n",
    "    return topo_res\n",
    "\n",
    "def prepare_balanced_data_unknown_pos_and_community(G, negative_ratio=1.0):\n",
    "    edges = list(G.edges())\n",
    "    nodes = list(G.nodes())\n",
    "    n_pos = len(edges)\n",
    "    data = []\n",
    "    random.seed(42)\n",
    "\n",
    "    # --- √âTAPE CRUCIALE : PR√â-CALCUL ---\n",
    "    # On calcule les m√©triques globales une seule fois ici\n",
    "    print(\"Pr√©-calcul des m√©triques de n≈ìuds...\")\n",
    "    precomputed = {\n",
    "        'pr': nx.pagerank(G),                    # PageRank (PR)\n",
    "        'lcc': nx.clustering(G),                # Local Clustering Coefficient (LCC)\n",
    "        'and': nx.average_neighbor_degree(G),   # Average Neighbor Degree (AND)\n",
    "        'dc': nx.degree_centrality(G)           # Degree Centrality (DC)\n",
    "    }\n",
    "    \n",
    "    # --- 1. CLASSE POSITIVE ---\n",
    "    for u, v in edges:\n",
    "        topo = get_topology_features(G, u, v, precomputed, is_existing_edge=True)\n",
    "        \n",
    "        row = {\n",
    "            'u': u, \n",
    "            'v': v,\n",
    "            'target': 1\n",
    "        }\n",
    "        row.update(topo)\n",
    "        data.append(row)\n",
    "    \n",
    "    # --- 2. CLASSE N√âGATIVE ---\n",
    "    n_neg_target = int(n_pos * negative_ratio)\n",
    "    neg_count = 0\n",
    "    while neg_count < n_neg_target:\n",
    "        u, v = random.sample(nodes, 2)\n",
    "        if not G.has_edge(u, v) and u != v:\n",
    "            topo = get_topology_features(G, u, v, precomputed, is_existing_edge=False)\n",
    "            \n",
    "            row = {\n",
    "                'u': u, \n",
    "                'v': v,\n",
    "                'target': 0\n",
    "            }\n",
    "            row.update(topo)\n",
    "            data.append(row)\n",
    "            neg_count += 1\n",
    "\n",
    "    print(f\"DataFrame cr√©√© <3 : {len(data)} paires de noeuds choisies\")\n",
    "    return pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "d1a1d50f-8cce-41e1-8915-aaa8dc7f039d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pr√©-calcul des m√©triques de n≈ìuds...\n",
      "DataFrame cr√©√© <3 : 27094 paires de noeuds choisies\n"
     ]
    }
   ],
   "source": [
    "heuristics_only_data = prepare_balanced_data_unknown_pos_and_community(G_real)\n",
    "heuristics_only_data.to_parquet(\"outputs/datasets/Airports_heuristics_only_dataset.parquet\", engine='pyarrow')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "976e7b43-edef-4692-a8b8-b48fc650b923",
   "metadata": {},
   "source": [
    "## 2 - Inf√©rences sur Position & Communities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff7fa35a-1f85-421b-b43a-37cf8614aefe",
   "metadata": {},
   "source": [
    "#### DeepWalk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384e6c54-fdc4-411d-83d7-1874d31c813e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(heuristics_only_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ba9df4-2ce9-41d9-8f32-825c6277a8fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "762354cf-5862-4d0e-8c2a-7875ed0656af",
   "metadata": {},
   "source": [
    "#### Node2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae0f791-b877-46c6-bfdd-101ab1040d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from node2vec import Node2Vec\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91526a52-d99d-4910-bac5-4100aaf9bab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_node2vec_features(G, dimensions=64):\n",
    "    \"\"\"\n",
    "    G√©n√®re les embeddings Node2Vec et retourne un dictionnaire {node_id: vector}\n",
    "    \"\"\"\n",
    "    print(f\"üöÄ G√©n√©ration des marches al√©atoires (dim={dimensions})...\")\n",
    "    \n",
    "    # Configuration de Node2Vec\n",
    "    # p=1, q=1 -> √©quivalent √† DeepWalk\n",
    "    # p=1, q=2 -> Favorise l'exploration locale (structure)\n",
    "    # p=2, q=0.5 -> Favorise l'exploration lointaine (communaut√©s) - homophilie\n",
    "    node2vec = Node2Vec(G, \n",
    "                        dimensions=dimensions, \n",
    "                        walk_length=30, \n",
    "                        num_walks=100, \n",
    "                        workers=4, \n",
    "                        p=1, q=1)\n",
    "\n",
    "    print(\"üß† Entra√Ænement du mod√®le Skip-gram...\")\n",
    "    model = node2vec.fit(window=10, min_count=1, batch_words=4)\n",
    "    \n",
    "    # On r√©cup√®re les vecteurs dans un dictionnaire\n",
    "    embeddings = {str(node): model.wv[str(node)] for node in G.nodes()}\n",
    "    return embeddings\n",
    "\n",
    "def add_node2vec_to_df(df, embeddings):\n",
    "    \"\"\"\n",
    "    Ajoute des features de similarit√© bas√©es sur Node2Vec au DataFrame\n",
    "    \"\"\"\n",
    "    print(\"üìä Calcul des distances vectorielles pour chaque paire...\")\n",
    "    \n",
    "    def get_cosine_sim(u, v):\n",
    "        vec_u = embeddings[str(u)].reshape(1, -1)\n",
    "        vec_v = embeddings[str(v)].reshape(1, -1)\n",
    "        return cosine_similarity(vec_u, vec_v)[0][0]\n",
    "\n",
    "    def get_l2_dist(u, v):\n",
    "        vec_u = embeddings[str(u)]\n",
    "        vec_v = embeddings[str(v)]\n",
    "        return np.linalg.norm(vec_u - vec_v)\n",
    "\n",
    "    # Similarit√© Cosinus : 1 = tr√®s proche, 0 = perpendiculaire\n",
    "    df['n2v_cosine'] = df.apply(lambda row: get_cosine_sim(row['u'], row['v']), axis=1)\n",
    "    \n",
    "    # Distance Euclidienne : plus c'est petit, plus ils sont proches\n",
    "    df['n2v_dist'] = df.apply(lambda row: get_l2_dist(row['u'], row['v']), axis=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# --- UTILISATION ---\n",
    "# 1. Calcul des vecteurs d'embedding\n",
    "n2v_embeddings = compute_node2vec_features(G_real)\n",
    "\n",
    "# 2. Injection dans le DataFrame de training/test\n",
    "node2vec_homophilie_p2_q0_5_data = add_node2vec_to_df(heuristics_only_data, n2v_embeddings)\n",
    "node2vec_homophilie_p2_q0_5_data.to_parquet(\n",
    "    \"outputs/datasets/Airports_deepwalk_p1_q1_dataset.parquet\", \n",
    "    engine='pyarrow'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dead6f1f-329d-4df3-ae27-bf576e881508",
   "metadata": {},
   "source": [
    "#### Role2Vec Tentative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54cfe3eb-46ad-461a-a567-d3afbeff4d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from karateclub import Role2Vec\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed29b30-b1c9-4758-9a84-e486393adea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Karate Club n√©cessite que les n≈ìuds soient index√©s par des entiers de 0 √† N-1\n",
    "mapping = {node: i for i, node in enumerate(G_real.nodes())}\n",
    "reverse_mapping = {i: node for node, i in mapping.items()}\n",
    "G_reindexed = nx.relabel_nodes(G_real, mapping)\n",
    "\n",
    "# 2. Entra√Ænement de Role2Vec\n",
    "print(\"üé≠ Calcul de Role2Vec (Roles structurels)...\")\n",
    "# On reste sur des dimensions coh√©rentes avec tes autres tests\n",
    "model_r2v = Role2Vec(dimensions=64, walk_number=10, walk_length=80) \n",
    "model_r2v.fit(G_reindexed)\n",
    "\n",
    "# 3. Extraction des vecteurs\n",
    "r2v_embeddings_raw = model_r2v.get_embedding()\n",
    "r2v_embeddings = {reverse_mapping[i]: r2v_embeddings_raw[i] for i in range(len(G_real))}\n",
    "\n",
    "print(\"‚úÖ Role2Vec termin√© !\")\n",
    "\n",
    "role2vec_data = add_node2vec_to_df(heuristics_only_data, r2v_embeddings)\n",
    "role2vec_data.to_parquet(\n",
    "    \"outputs/datasets/Airports_role2vec_dataset.parquet\", \n",
    "    engine='pyarrow'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7b4a4d-a86d-412f-90bd-a5d4726e0d83",
   "metadata": {},
   "source": [
    "#### Louvain Communities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc24c9dc-1a20-411e-89cb-612d877c3d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from networkx.algorithms.community import louvain_communities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "fb2dc25b-b0bc-4b68-9854-778008dd8032",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3363\n",
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "communities = nx.community.louvain_communities(G_real, seed=42)\n",
    "\n",
    "node_to_community = {} \n",
    "for i, community in enumerate(communities):\n",
    "    for node in community:\n",
    "        node_to_community[node] = i\n",
    "        \n",
    "print(len(node_to_community))\n",
    "\n",
    "heuristics_only_data = pd.read_parquet(\"outputs/datasets/Airports_heuristics_only_dataset.parquet\")\n",
    "print(type(heuristics_only_data))\n",
    "louvain_communities_data = heuristics_only_data.copy()\n",
    "\n",
    "louvain_communities_data[\"community_u\"] = louvain_communities_data[\"u\"].map(node_to_community)\n",
    "louvain_communities_data[\"community_v\"] = louvain_communities_data[\"v\"].map(node_to_community)\n",
    "louvain_communities_data[\"same_community\"] = (louvain_communities_data[\"community_u\"] == louvain_communities_data[\"community_v\"]).astype(int)\n",
    "\n",
    "louvain_communities_data.to_parquet(\n",
    "    \"outputs/datasets/Airports_louvain_communities_dataset.parquet\", \n",
    "    engine='pyarrow'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5155a5e8-1a6d-454a-a87e-2b4e6dbe929b",
   "metadata": {},
   "source": [
    "#### Infomap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f7bd9315-4246-49af-b623-ae0c179498c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from infomap import Infomap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "c3d4927f-ecf0-4efa-85ef-74b9483dd126",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Infomap a trouv√© 179 communaut√©s.\n",
      "<class 'dict'>\n",
      "   u  v  target  cn        aa        jc   pa  sp      pr_u      pr_v  lcc_u  \\\n",
      "0  0  1       1   1  0.303413  0.200000    8   2  0.000156  0.000293    1.0   \n",
      "1  0  2       1   1  0.721348  0.035714   54   2  0.000156  0.001552    1.0   \n",
      "2  1  2       1   3  4.328085  0.107143  108   2  0.000293  0.001552    0.5   \n",
      "\n",
      "      lcc_v  and_u      and_v      dc_u      dc_v  infomap_u  infomap_v  \\\n",
      "0  0.500000  15.50   8.250000  0.000595  0.001190         48         48   \n",
      "1  0.096866  15.50  18.962963  0.000595  0.008031         48         48   \n",
      "2  0.096866   8.25  18.962963  0.001190  0.008031         48         48   \n",
      "\n",
      "   same_infomap  \n",
      "0             1  \n",
      "1             1  \n",
      "2             1  \n",
      "Type cl√© dico: <class 'int'>\n",
      "Type colonne DF: int64\n"
     ]
    }
   ],
   "source": [
    "def get_infomap_communities(G):\n",
    "    im = Infomap(\"--two-level --silent\")\n",
    "    \n",
    "    for source, target in G.edges():\n",
    "        im.add_link(int(source), int(target))\n",
    "    \n",
    "    # Ex√©cution\n",
    "    im.run()\n",
    "    \n",
    "    # Extraction des r√©sultats\n",
    "    # node.node_id est l'ID d'origine, node.module_id est le cluster\n",
    "    node_to_infomap = {node.node_id: node.module_id for node in im.tree if node.is_leaf}\n",
    "    return node_to_infomap\n",
    "\n",
    "# Utilisation\n",
    "infomap_dict = get_infomap_communities(G_real)\n",
    "print(f\"Infomap a trouv√© {len(set(infomap_dict.values()))} communaut√©s.\")\n",
    "\n",
    "infomap_data = heuristics_only_data.copy()\n",
    "\n",
    "# 2. Mapping avec le dictionnaire Infomap\n",
    "infomap_data[\"u\"] = pd.to_numeric(infomap_data[\"u\"], errors='coerce').astype(int)\n",
    "infomap_data[\"v\"] = pd.to_numeric(infomap_data[\"v\"], errors='coerce').astype(int)\n",
    "\n",
    "infomap_data[\"infomap_u\"] = infomap_data[\"u\"].map(infomap_dict)\n",
    "infomap_data[\"infomap_v\"] = infomap_data[\"v\"].map(infomap_dict)\n",
    "infomap_data[\"same_infomap\"] = (infomap_data[\"infomap_u\"] == infomap_data[\"infomap_v\"]).astype(int)\n",
    "\n",
    "print(type(infomap_dict))\n",
    "print(infomap_data.head(3))\n",
    "\n",
    "print(f\"Type cl√© dico: {type(list(infomap_dict.keys())[0])}\")\n",
    "\n",
    "# V√©rifie le type de la colonne du DataFrame\n",
    "print(f\"Type colonne DF: {infomap_data['u'].dtype}\")\n",
    "# 4. Sauvegarde\n",
    "infomap_data.to_parquet(\"outputs/datasets/Airports_infomap_dataset.parquet\", engine='pyarrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5a2ff4-2020-4900-93fd-81fde996b705",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d31d326-940e-4fb4-bf7f-6dc2d3f13307",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e98eff5-e452-4aa0-9780-8e346b64ca3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0964fd24-7c75-4275-bf02-c5d40573649f",
   "metadata": {},
   "source": [
    "## 3 - Comparaison des perfs sur XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "4084c9a1-42ab-443a-8b49-70c5964c27bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import f1_score, confusion_matrix, roc_auc_score, average_precision_score\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "e75bdf93-2b51-4d7e-af46-f440cb67b882",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Tous les datasets ont √©t√© charg√©s avec succ√®s depuis le format Parquet.\n",
      "\n",
      "--- Dataset: Heuristics Only ---\n",
      "   u  v  target  cn        aa        jc   pa  sp      pr_u      pr_v  lcc_u  \\\n",
      "0  0  1       1   1  0.303413  0.200000    8   2  0.000156  0.000293    1.0   \n",
      "1  0  2       1   1  0.721348  0.035714   54   2  0.000156  0.001552    1.0   \n",
      "2  1  2       1   3  4.328085  0.107143  108   2  0.000293  0.001552    0.5   \n",
      "3  1  3       1   1  0.303413  0.200000    8   2  0.000293  0.000156    0.5   \n",
      "4  1  4       1   1  0.303413  0.200000    8   2  0.000293  0.000156    0.5   \n",
      "\n",
      "      lcc_v  and_u      and_v      dc_u      dc_v  \n",
      "0  0.500000  15.50   8.250000  0.000595  0.001190  \n",
      "1  0.096866  15.50  18.962963  0.000595  0.008031  \n",
      "2  0.096866   8.25  18.962963  0.001190  0.008031  \n",
      "3  1.000000   8.25  15.500000  0.001190  0.000595  \n",
      "4  1.000000   8.25  15.500000  0.001190  0.000595  \n",
      "\n",
      "--- Dataset: DeepWalk ---\n",
      "   u  v  target  cn        aa        jc   pa  sp      pr_u      pr_v  lcc_u  \\\n",
      "0  0  1       1   1  0.303413  0.200000    8   2  0.000156  0.000293    1.0   \n",
      "1  0  2       1   1  0.721348  0.035714   54   2  0.000156  0.001552    1.0   \n",
      "2  1  2       1   3  4.328085  0.107143  108   2  0.000293  0.001552    0.5   \n",
      "3  1  3       1   1  0.303413  0.200000    8   2  0.000293  0.000156    0.5   \n",
      "4  1  4       1   1  0.303413  0.200000    8   2  0.000293  0.000156    0.5   \n",
      "\n",
      "      lcc_v  and_u      and_v      dc_u      dc_v  n2v_cosine  n2v_dist  \n",
      "0  0.500000  15.50   8.250000  0.000595  0.001190    0.962876  1.133980  \n",
      "1  0.096866  15.50  18.962963  0.000595  0.008031    0.938585  1.528418  \n",
      "2  0.096866   8.25  18.962963  0.001190  0.008031    0.933306  1.588104  \n",
      "3  1.000000   8.25  15.500000  0.001190  0.000595    0.977782  0.880874  \n",
      "4  1.000000   8.25  15.500000  0.001190  0.000595    0.967007  1.065983  \n",
      "\n",
      "--- Dataset: N2V Homophilie ---\n",
      "   u  v  target  cn        aa        jc   pa  sp      pr_u      pr_v  lcc_u  \\\n",
      "0  0  1       1   1  0.303413  0.200000    8   2  0.000156  0.000293    1.0   \n",
      "1  0  2       1   1  0.721348  0.035714   54   2  0.000156  0.001552    1.0   \n",
      "2  1  2       1   3  4.328085  0.107143  108   2  0.000293  0.001552    0.5   \n",
      "3  1  3       1   1  0.303413  0.200000    8   2  0.000293  0.000156    0.5   \n",
      "4  1  4       1   1  0.303413  0.200000    8   2  0.000293  0.000156    0.5   \n",
      "\n",
      "      lcc_v  and_u      and_v      dc_u      dc_v  n2v_cosine  n2v_dist  \n",
      "0  0.500000  15.50   8.250000  0.000595  0.001190    0.978779  0.837193  \n",
      "1  0.096866  15.50  18.962963  0.000595  0.008031    0.944667  1.373465  \n",
      "2  0.096866   8.25  18.962963  0.001190  0.008031    0.943019  1.448473  \n",
      "3  1.000000   8.25  15.500000  0.001190  0.000595    0.978821  0.837158  \n",
      "4  1.000000   8.25  15.500000  0.001190  0.000595    0.981605  0.781163  \n",
      "\n",
      "--- Dataset: N2V Structural ---\n",
      "   u  v  target  cn        aa        jc   pa  sp      pr_u      pr_v  lcc_u  \\\n",
      "0  0  1       1   1  0.303413  0.200000    8   2  0.000156  0.000293    1.0   \n",
      "1  0  2       1   1  0.721348  0.035714   54   2  0.000156  0.001552    1.0   \n",
      "2  1  2       1   3  4.328085  0.107143  108   2  0.000293  0.001552    0.5   \n",
      "3  1  3       1   1  0.303413  0.200000    8   2  0.000293  0.000156    0.5   \n",
      "4  1  4       1   1  0.303413  0.200000    8   2  0.000293  0.000156    0.5   \n",
      "\n",
      "      lcc_v  and_u      and_v      dc_u      dc_v  n2v_cosine  n2v_dist  \n",
      "0  0.500000  15.50   8.250000  0.000595  0.001190    0.974769  0.968239  \n",
      "1  0.096866  15.50  18.962963  0.000595  0.008031    0.941745  1.522043  \n",
      "2  0.096866   8.25  18.962963  0.001190  0.008031    0.939153  1.619745  \n",
      "3  1.000000   8.25  15.500000  0.001190  0.000595    0.968604  1.078399  \n",
      "4  1.000000   8.25  15.500000  0.001190  0.000595    0.974173  0.980431  \n",
      "\n",
      "--- Dataset: Role2Vec ---\n",
      "   u  v  target  cn        aa        jc   pa  sp      pr_u      pr_v  lcc_u  \\\n",
      "0  0  1       1   1  0.303413  0.200000    8   2  0.000156  0.000293    1.0   \n",
      "1  0  2       1   1  0.721348  0.035714   54   2  0.000156  0.001552    1.0   \n",
      "2  1  2       1   3  4.328085  0.107143  108   2  0.000293  0.001552    0.5   \n",
      "3  1  3       1   1  0.303413  0.200000    8   2  0.000293  0.000156    0.5   \n",
      "4  1  4       1   1  0.303413  0.200000    8   2  0.000293  0.000156    0.5   \n",
      "\n",
      "      lcc_v  and_u      and_v      dc_u      dc_v  n2v_cosine  n2v_dist  \n",
      "0  0.500000  15.50   8.250000  0.000595  0.001190    0.994449  1.202858  \n",
      "1  0.096866  15.50  18.962963  0.000595  0.008031    0.951035  3.937577  \n",
      "2  0.096866   8.25  18.962963  0.001190  0.008031    0.916918  4.718184  \n",
      "3  1.000000   8.25  15.500000  0.001190  0.000595    0.993611  1.864625  \n",
      "4  1.000000   8.25  15.500000  0.001190  0.000595    0.970031  3.558781  \n",
      "\n",
      "--- Dataset: Louvain Communities ---\n",
      "   u  v  target  cn        aa        jc   pa  sp      pr_u      pr_v  lcc_u  \\\n",
      "0  0  1       1   1  0.303413  0.200000    8   2  0.000156  0.000293    1.0   \n",
      "1  0  2       1   1  0.721348  0.035714   54   2  0.000156  0.001552    1.0   \n",
      "2  1  2       1   3  4.328085  0.107143  108   2  0.000293  0.001552    0.5   \n",
      "3  1  3       1   1  0.303413  0.200000    8   2  0.000293  0.000156    0.5   \n",
      "4  1  4       1   1  0.303413  0.200000    8   2  0.000293  0.000156    0.5   \n",
      "\n",
      "      lcc_v  and_u      and_v      dc_u      dc_v  community_u  community_v  \\\n",
      "0  0.500000  15.50   8.250000  0.000595  0.001190            2            2   \n",
      "1  0.096866  15.50  18.962963  0.000595  0.008031            2            2   \n",
      "2  0.096866   8.25  18.962963  0.001190  0.008031            2            2   \n",
      "3  1.000000   8.25  15.500000  0.001190  0.000595            2            2   \n",
      "4  1.000000   8.25  15.500000  0.001190  0.000595            2            2   \n",
      "\n",
      "   same_community  \n",
      "0               1  \n",
      "1               1  \n",
      "2               1  \n",
      "3               1  \n",
      "4               1  \n",
      "\n",
      "--- Dataset: Infomap Communities ---\n",
      "   u  v  target  cn        aa        jc   pa  sp      pr_u      pr_v  lcc_u  \\\n",
      "0  0  1       1   1  0.303413  0.200000    8   2  0.000156  0.000293    1.0   \n",
      "1  0  2       1   1  0.721348  0.035714   54   2  0.000156  0.001552    1.0   \n",
      "2  1  2       1   3  4.328085  0.107143  108   2  0.000293  0.001552    0.5   \n",
      "3  1  3       1   1  0.303413  0.200000    8   2  0.000293  0.000156    0.5   \n",
      "4  1  4       1   1  0.303413  0.200000    8   2  0.000293  0.000156    0.5   \n",
      "\n",
      "      lcc_v  and_u      and_v      dc_u      dc_v  infomap_u  infomap_v  \\\n",
      "0  0.500000  15.50   8.250000  0.000595  0.001190         48         48   \n",
      "1  0.096866  15.50  18.962963  0.000595  0.008031         48         48   \n",
      "2  0.096866   8.25  18.962963  0.001190  0.008031         48         48   \n",
      "3  1.000000   8.25  15.500000  0.001190  0.000595         48         48   \n",
      "4  1.000000   8.25  15.500000  0.001190  0.000595         48         48   \n",
      "\n",
      "   same_infomap  \n",
      "0             1  \n",
      "1             1  \n",
      "2             1  \n",
      "3             1  \n",
      "4             1  \n",
      "\n",
      "--- Dataset: Homophilie and Louvain ---\n",
      "   u  v  target  cn        aa        jc   pa  sp      pr_u      pr_v  ...  \\\n",
      "0  0  1       1   1  0.303413  0.200000    8   2  0.000156  0.000293  ...   \n",
      "1  0  2       1   1  0.721348  0.035714   54   2  0.000156  0.001552  ...   \n",
      "2  1  2       1   3  4.328085  0.107143  108   2  0.000293  0.001552  ...   \n",
      "3  1  3       1   1  0.303413  0.200000    8   2  0.000293  0.000156  ...   \n",
      "4  1  4       1   1  0.303413  0.200000    8   2  0.000293  0.000156  ...   \n",
      "\n",
      "      lcc_v  and_u      and_v      dc_u      dc_v  n2v_cosine  n2v_dist  \\\n",
      "0  0.500000  15.50   8.250000  0.000595  0.001190    0.978779  0.837193   \n",
      "1  0.096866  15.50  18.962963  0.000595  0.008031    0.944667  1.373465   \n",
      "2  0.096866   8.25  18.962963  0.001190  0.008031    0.943019  1.448473   \n",
      "3  1.000000   8.25  15.500000  0.001190  0.000595    0.978821  0.837158   \n",
      "4  1.000000   8.25  15.500000  0.001190  0.000595    0.981605  0.781163   \n",
      "\n",
      "   community_u  community_v  same_community  \n",
      "0            2            2               1  \n",
      "1            2            2               1  \n",
      "2            2            2               1  \n",
      "3            2            2               1  \n",
      "4            2            2               1  \n",
      "\n",
      "[5 rows x 21 columns]\n",
      "\n",
      "--- Dataset: Homophilie & Structural ---\n",
      "   u  v  target  cn        aa        jc   pa  sp      pr_u      pr_v  lcc_u  \\\n",
      "0  0  1       1   1  0.303413  0.200000    8   2  0.000156  0.000293    1.0   \n",
      "1  0  2       1   1  0.721348  0.035714   54   2  0.000156  0.001552    1.0   \n",
      "2  1  2       1   3  4.328085  0.107143  108   2  0.000293  0.001552    0.5   \n",
      "3  1  3       1   1  0.303413  0.200000    8   2  0.000293  0.000156    0.5   \n",
      "4  1  4       1   1  0.303413  0.200000    8   2  0.000293  0.000156    0.5   \n",
      "\n",
      "      lcc_v  and_u      and_v      dc_u      dc_v  n2v_cosine  n2v_dist  \\\n",
      "0  0.500000  15.50   8.250000  0.000595  0.001190    0.978779  0.837193   \n",
      "1  0.096866  15.50  18.962963  0.000595  0.008031    0.944667  1.373465   \n",
      "2  0.096866   8.25  18.962963  0.001190  0.008031    0.943019  1.448473   \n",
      "3  1.000000   8.25  15.500000  0.001190  0.000595    0.978821  0.837158   \n",
      "4  1.000000   8.25  15.500000  0.001190  0.000595    0.981605  0.781163   \n",
      "\n",
      "   n2v_cosine_s  n2v_dist_s  \n",
      "0      0.974769    0.968239  \n",
      "1      0.941745    1.522043  \n",
      "2      0.939153    1.619745  \n",
      "3      0.968604    1.078399  \n",
      "4      0.974173    0.980431  \n"
     ]
    }
   ],
   "source": [
    "# Chargement des datasets au format Parquet\n",
    "heuristics_only_data = pd.read_parquet(\"outputs/datasets/Airports_heuristics_only_dataset.parquet\")\n",
    "deepwalk_data = pd.read_parquet(\"outputs/datasets/Airports_deepwalk_p1_q1_dataset.parquet\")\n",
    "node2vec_homophilie_p2_q0_5_data = pd.read_parquet(\"outputs/datasets/Airports_node2vec_homophilie_p2_q0_5dataset.parquet\")\n",
    "node2vec_structural_p1_q2_data = pd.read_parquet(\"outputs/datasets/Airports_node2vec_structural_p1_q2dataset.parquet\")\n",
    "role2vec_data = pd.read_parquet(\"outputs/datasets/Airports_role2vec_dataset.parquet\")\n",
    "\n",
    "louvain_communities_data = pd.read_parquet(\"outputs/datasets/Airports_louvain_communities_dataset.parquet\")\n",
    "infomap_data = pd.read_parquet(\"outputs/datasets/Airports_infomap_dataset.parquet\")\n",
    "\n",
    "homo_and_louvain_data = node2vec_homophilie_p2_q0_5_data.copy()\n",
    "\n",
    "homo_and_louvain_data[\"community_u\"] = homo_and_louvain_data[\"u\"].map(node_to_community)\n",
    "homo_and_louvain_data[\"community_v\"] = homo_and_louvain_data[\"v\"].map(node_to_community)\n",
    "homo_and_louvain_data[\"same_community\"] = (homo_and_louvain_data[\"community_u\"] == homo_and_louvain_data[\"community_v\"]).astype(int)\n",
    "\n",
    "homo_and_structural_data = node2vec_homophilie_p2_q0_5_data.copy()\n",
    "homo_and_structural_data[\"n2v_cosine_s\"] = node2vec_structural_p1_q2_data[\"n2v_cosine\"]\n",
    "homo_and_structural_data[\"n2v_dist_s\"] = node2vec_structural_p1_q2_data[\"n2v_dist\"]\n",
    "\n",
    "\n",
    "print(\"‚úÖ Tous les datasets ont √©t√© charg√©s avec succ√®s depuis le format Parquet.\")\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "my_bench_datasets = {\n",
    "    \"Heuristics Only\": heuristics_only_data,\n",
    "    \"DeepWalk\": deepwalk_data,\n",
    "    \"N2V Homophilie\": node2vec_homophilie_p2_q0_5_data,\n",
    "    \"N2V Structural\": node2vec_structural_p1_q2_data,\n",
    "    \"Role2Vec\": role2vec_data,\n",
    "    \"Louvain Communities\": louvain_communities_data,\n",
    "    \"Infomap Communities\": infomap_data,\n",
    "    \"Homophilie and Louvain\" : homo_and_louvain_data,\n",
    "    \"Homophilie & Structural\" : homo_and_structural_data\n",
    "}\n",
    "\n",
    "for name, dataset in my_bench_datasets.items():\n",
    "    print(f\"\\n--- Dataset: {name} ---\")\n",
    "    print(dataset.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "1689cf31-b7e5-4b07-b6d7-d8b8cf57165b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traitement de Heuristics Only...\n",
      "Traitement de DeepWalk...\n",
      "Traitement de N2V Homophilie...\n",
      "Traitement de N2V Structural...\n",
      "Traitement de Role2Vec...\n",
      "Traitement de Louvain Communities...\n",
      "Traitement de Infomap Communities...\n",
      "Traitement de Homophilie and Louvain...\n",
      "Traitement de Homophilie & Structural...\n",
      "\n",
      " R√âSULTATS COMPARATIFS\n",
      "                Dataset  AUC-ROC  F1-Score  VP (True Pos)  VN (True Neg)  FP (False Pos)  FN (False Neg)  Hits@50\n",
      "               DeepWalk   0.9999    0.9978           2709           2698              12               0      1.0\n",
      "         N2V Homophilie   0.9999    0.9980           2709           2699              11               0      1.0\n",
      " Homophilie and Louvain   0.9999    0.9980           2709           2699              11               0      1.0\n",
      "Homophilie & Structural   0.9999    0.9980           2709           2699              11               0      1.0\n",
      "         N2V Structural   0.9998    0.9976           2709           2697              13               0      1.0\n",
      "               Role2Vec   0.9995    0.9917           2692           2682              28              17      1.0\n",
      "    Infomap Communities   0.9987    0.9877           2684           2668              42              25      1.0\n",
      "    Louvain Communities   0.9986    0.9879           2693           2660              50              16      1.0\n",
      "        Heuristics Only   0.9985    0.9866           2683           2663              47              26      1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, confusion_matrix, roc_auc_score, average_precision_score\n",
    "\n",
    "def run_benchmark(datasets_dict, K=50):\n",
    "    results = []\n",
    "    \n",
    "    # Colonnes techniques √† exclure\n",
    "    exclude = {'u', 'v', 'target'}\n",
    "    \n",
    "    for name, df in datasets_dict.items():\n",
    "        print(f\"Traitement de {name}...\")\n",
    "        \n",
    "        features = [c for c in df.columns if c not in exclude]\n",
    "        df_tmp = df.dropna(subset=features + ['target'])\n",
    "        X = df_tmp[features]\n",
    "        y = df_tmp['target']\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=0.2, random_state=42, stratify=y\n",
    "        )\n",
    "        \n",
    "        model = XGBClassifier(\n",
    "            n_estimators=100,\n",
    "            learning_rate=0.1,\n",
    "            max_depth=6,\n",
    "            eval_metric='logloss',\n",
    "            tree_method='hist',\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        \n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Pr√©dictions\n",
    "        probs = model.predict_proba(X_test)[:, 1]\n",
    "        preds = model.predict(X_test)  # Seuil par d√©faut √† 0.5\n",
    "        \n",
    "        # M√©triques de performance\n",
    "        auc_roc = roc_auc_score(y_test, probs)\n",
    "        f1 = f1_score(y_test, preds)\n",
    "        \n",
    "        # Matrice de confusion\n",
    "        tn, fp, fn, tp = confusion_matrix(y_test, preds).ravel()\n",
    "        \n",
    "        # Hits@K\n",
    "        top_k_indices = np.argsort(probs)[-K:]\n",
    "        hits_at_k = y_test.iloc[top_k_indices].sum() / K\n",
    "        \n",
    "        results.append({\n",
    "            'Dataset': name,\n",
    "            'AUC-ROC': round(auc_roc, 4),\n",
    "            'F1-Score': round(f1, 4),\n",
    "            'VP (True Pos)': tp,\n",
    "            'VN (True Neg)': tn,\n",
    "            'FP (False Pos)': fp,\n",
    "            'FN (False Neg)': fn,\n",
    "            f'Hits@{K}': round(hits_at_k, 4)\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(results).sort_values(by='AUC-ROC', ascending=False)\n",
    "\n",
    "# --- EXECUTION ---\n",
    "summary = run_benchmark(my_bench_datasets)\n",
    "print(\"\\n R√âSULTATS COMPARATIFS\")\n",
    "print(summary.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4302fbd-2ae4-42c6-9e56-4db4514ab523",
   "metadata": {},
   "source": [
    "## 4 - Analyse SHAP rapide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b12a0bf-9db2-4af1-895c-2af65572c233",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be34f4dc-c3a3-4608-a2aa-db8ddcf1d93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_with_shap(model, X_test, output_dir=\"outputs/plots\"):\n",
    "    \"\"\"Calcule les SHAP values et g√©n√®re les plots globaux proprement.\"\"\"\n",
    "    # 1. Configuration de l'explainer 'Bo√Æte Noire' (le plus stable sur mon Mac)\n",
    "    # On d√©finit la fonction de pr√©diction (proba de la classe 1)\n",
    "    model_predict = lambda x: model.predict_proba(x)[:, 1]\n",
    "    \n",
    "    # Utilisation d'un masker (√©chantillon de r√©f√©rence)\n",
    "    # On prend 50 lignes pour √©quilibrer vitesse et pr√©cision\n",
    "    masker = X_test.iloc[:50]\n",
    "    \n",
    "    # Initialisation de l'explainer\n",
    "    explainer = shap.Explainer(model_predict, masker)    \n",
    "    \n",
    "    # 2. Calcul effectif des SHAP values\n",
    "    # On r√©cup√®re l'objet 'Explanation' complet\n",
    "    shap_explanation = explainer(X_test)\n",
    "    \n",
    "    # 3. Extraction des valeurs num√©riques pour le retour de fonction\n",
    "    # On r√©cup√®re les valeurs brutes (.values)\n",
    "    shap_values = shap_explanation.values\n",
    "\n",
    "    # Gestion de la dimension (si SHAP renvoie [n_samples, n_features, 2])\n",
    "    if len(shap_values.shape) == 3:\n",
    "        shap_values = shap_values[:, :, 1]\n",
    "    \n",
    "    # --- G√âN√âRATION DES PLOTS ---\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Plot 1: Summary Points (Beeswarm)\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    # On peut passer l'objet explanation directement, c'est plus moderne\n",
    "    shap.plots.beeswarm(shap_explanation, show=False)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, \"shap_summary_points.png\"))\n",
    "    plt.close()\n",
    "\n",
    "    # Plot 2: Summary Bar\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    shap.plots.bar(shap_explanation, show=False)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, \"shap_summary_bar.png\"))\n",
    "    plt.close()\n",
    "    \n",
    "    return shap_values\n",
    "\n",
    "def calculate_feature_rankings(shap_values, feature_names, output_dir=\"outputs/plots\"):\n",
    "    \"\"\"Calcule la distribution des rangs et g√©n√®re le barplot du Top 5.\"\"\"\n",
    "    abs_shap = np.abs(shap_values)\n",
    "    ranks = np.argsort(-abs_shap, axis=1)\n",
    "    \n",
    "    ranking_stats = {}\n",
    "    n_samples, n_features = shap_values.shape\n",
    "\n",
    "    for i, name in enumerate(feature_names):\n",
    "        feature_ranks = np.where(ranks == i)[1] + 1\n",
    "        counts = np.bincount(feature_ranks, minlength=n_features + 1)[1:]\n",
    "        ranking_stats[name] = (counts / n_samples) * 100\n",
    "\n",
    "    df_ranks = pd.DataFrame(ranking_stats, index=[f\"Rang {i+1}\" for i in range(n_features)])\n",
    "    \n",
    "    # Plot 3: Top 5 Appearance\n",
    "    top5 = df_ranks.iloc[0:5, :].sum(axis=0).sort_values(ascending=False)\n",
    "    plt.figure(figsize=(12, 7))\n",
    "    sns.barplot(x=top5.index, y=top5.values, palette=\"viridis\")\n",
    "    plt.title(\"Importance structurelle : % de pr√©sence dans le Top 5 SHAP\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, \"shap_top5_frequency.png\"))\n",
    "    plt.close()\n",
    "    \n",
    "    return df_ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5c88cc-321a-4a41-8789-fffe0ca4b06a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c51237-c9f0-4f35-9ac3-40540faab945",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Stage M2)",
   "language": "python",
   "name": "stagem2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
